{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rogue\n",
      "  Downloading rogue-0.0.2.tar.gz (5.4 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: rogue\n",
      "  Building wheel for rogue (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rogue: filename=rogue-0.0.2-py3-none-any.whl size=7297 sha256=6808dac50be5d0d37feb5fa8cb17a30e74128401ce64d23323fa9c4e584b1635\n",
      "  Stored in directory: /Users/ajeyk/Library/Caches/pip/wheels/88/65/0c/e2d3efe66c4b48cb42ed2a2c5b310b9b5884c42238096f4414\n",
      "Successfully built rogue\n",
      "Installing collected packages: rogue\n",
      "Successfully installed rogue-0.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.stats import entropy\n",
    "# from rogue import Rogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/ajeyk/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load DUC dataset or 20NG dataset (you can modify as needed)\n",
    "documents = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "documents = documents.data\n",
    "\n",
    "\n",
    "# Preprocess: Sentence segmentation & tokenization\n",
    "def preprocess_text(text):\n",
    "    return [word.lower() for word in word_tokenize(text) if word.isalnum]\n",
    "\n",
    "\n",
    "# Segment documents into sentences\n",
    "documents_sentences = [sent_tokenize(doc) for doc in documents]\n",
    "flattened_sentences = [sent for doc in documents_sentences for sent in doc]\n",
    "\n",
    "# Compute PD (Document Distribution) - Word-based\n",
    "def compute_word_distribution(doc_sentences):\n",
    "    word_counts = Counter()\n",
    "    for sent in doc_sentences:\n",
    "        word_counts.update(preprocess_text(sent))\n",
    "    \n",
    "    total_words = sum(word_counts.values())\n",
    "    return {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "\n",
    "# KL - divergence function\n",
    "def kl_divergence(p_dist, q_dist):\n",
    "    p_vals = np.array(list(p_dist.values()))\n",
    "    q_vals = np.array([q_dist.get(k, 1e-10) for k in p_dist.keys()])\n",
    "    return entropy(p_vals, q_vals)\n",
    "\n",
    "\n",
    "# KL divergence function\n",
    "def kl_sum_word_based(doc_sentences, summary_length=3):\n",
    "    PD = compute_word_distribution(doc_sentences)\n",
    "    PS = {}  # Growing summary distribution\n",
    "    summary = []\n",
    "\n",
    "    while len(summary) < summary_length:\n",
    "        best_sentence = None\n",
    "        best_divergence = float('inf')\n",
    "\n",
    "        for sentence in doc_sentences:\n",
    "            new_summary = summary + [sentence]\n",
    "            new_PS = compute_word_distribution(new_summary)\n",
    "            divergence = kl_divergence(new_PS, PD)\n",
    "\n",
    "            if divergence < best_divergence:\n",
    "                best_divergence = divergence\n",
    "                best_sentence = sentence\n",
    "        \n",
    "        if best_sentence:\n",
    "            summary.append(best_sentence)\n",
    "            doc_sentences.remove(best_sentence)\n",
    "\n",
    "    return \" \".join(summary)\n",
    "\n",
    "\n",
    "# KL-Sum Summarization over topics using LDA\n",
    "def kl_sum_topic_based(doc_sentences, dictionary, lda_model, summary_length=3):\n",
    "    # Compute PD (Document topic distribution)\n",
    "    bow_corpus = [dictionary.doc2bow(preprocess_text(sent)) for sent in doc_sentences]\n",
    "    # PD = np.mean([dict(lda_model[doc]) for doc in bow_corpus], axis=0)\n",
    "    # Initialize a dictionary to store topic distributions\n",
    "    topic_sums = defaultdict(float)\n",
    "    num_docs = len(bow_corpus)\n",
    "\n",
    "    # Accumulate topic probabilities\n",
    "    for doc in bow_corpus:\n",
    "        for topic_id, prob in lda_model[doc]:  # lda_model[doc] returns list of (topic_id, probability)\n",
    "            topic_sums[topic_id] += prob\n",
    "\n",
    "    # Compute the mean topic distribution (PD)\n",
    "    PD = {topic_id: topic_sums[topic_id] / num_docs for topic_id in topic_sums}\n",
    "    PD_array = np.array(list(PD.values()))\n",
    "\n",
    "    summary = []\n",
    "    PS = np.zeros_like(PD_array)\n",
    "\n",
    "    while len(summary) < summary_length:\n",
    "        best_sentence = None\n",
    "        best_divergence = float('inf')\n",
    "\n",
    "        for sentence in doc_sentences:\n",
    "            sent_bow = dictionary.doc2bow(preprocess_text(sentence))\n",
    "            sent_topic_dist = dict(lda_model[sent_bow])\n",
    "\n",
    "            new_PS = (PS * len(summary) + np.array([sent_topic_dist.get(k, 1e-10) for k in range(len(PD))])) / (len(summary) + 1)\n",
    "            divergence = entropy(new_PS, PD_array)\n",
    "\n",
    "            if divergence < best_divergence:\n",
    "                best_divergence = divergence\n",
    "                best_sentence = sentence\n",
    "\n",
    "        if best_sentence:\n",
    "            summary.append(best_sentence)\n",
    "            doc_sentences.remove(best_sentence)\n",
    "\n",
    "    return \" \".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extractive Summary (Word-based):\n",
      "Aykut Atalay Atakan -ciao +=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=+\n",
      "+=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=+\n"
     ]
    }
   ],
   "source": [
    "# Run KL-Sum over words\n",
    "summary_words = kl_sum_word_based(flattened_sentences)\n",
    "print(\"\\nExtractive Summary (Word-based):\")\n",
    "print(summary_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KL-summary over topics using LDA\n",
    "dictionary = Dictionary([preprocess_text(sent) for sent in flattened_sentences])\n",
    "corpus = [dictionary.doc2bow(preprocess_text(sent)) for sent in flattened_sentences]\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extractive Summary for Topic 0:\n",
      "He spoke\n",
      " to the racial heart strings of the German, opened the \n",
      " fountain of his national genius, strock down the spirit\n",
      " of defeatism...At no period since the World War had Berlin\n",
      " conducted so realistic, well organized, and planned policy\n",
      " as now, since Hitlers assumption to power...And whatever\n",
      " others may think concerning Hitlerism and Fascism as a \n",
      " system of Government, it is proved that they have revitalized\n",
      " and regenerated the two states, Germany and Italy.2\n",
      "\n",
      "1 Captain George Haig, The Case of Palestine, in Hairenik\n",
      "    Weekly, Friday, September 25, 1936.\n",
      "He\n",
      "also noted that the software people were starting to feel management\n",
      "pressure to cut corners, but hadnt had to give in to it much yet.\n",
      "\n",
      "Extractive Summary for Topic 1:\n",
      "Beware of our materialistic, worldly and selfish motives.\n",
      "Quoth the Moderator\n",
      "\n",
      "\n",
      "In a short poem God in His mercy made  the fixed pains of Hell,\n",
      "C. S. Lewis expresses an idea that Im sure was current among others,\n",
      "but I havent be able to find its source\n",
      "\n",
      "that even Hell is an expression of mercy, because God limits the amount\n",
      "of separation from Him, and hence the amount of agony, that one can\n",
      "achieve.\n",
      "\n",
      "Extractive Summary for Topic 2:\n",
      "arrow      2.50 056  Fair      Top secret module missing  Bard tunes  \n",
      "real maps\n",
      "TN.DE7     3.0  073  Good       Forest of Doom module detached but \n",
      "included  inner planes\n",
      "ykchev     1.75 074  Good      Combat computer missing  4 dragons \n",
      "TN.DE7     1.50 078  GoodVG    Monsters  aquatic ADD module detached \n",
      "but included  Language lesson\n",
      "geoffrey   1.50 079  GoodVG    top secret module detached but \n",
      "included  magic resistance\n",
      "mayla      2.50 081  Fair       High level ADD module detached but \n",
      "included  poison  material spell components\n",
      "UCCXKVB    2.00 082  Very Fair Baton races game insert missing  spell \n",
      "research\n",
      "TN.DE7     1.50 083  GoodVG    Babba Yaggas Hut module detached but \n",
      "included  unarmed combat\n",
      "geoffrey   1.50 084  Fair      Cover missing  Twofold talisman module\n",
      "squidly    1.75 085  Good       Twofold Talisman module  Clerics\n",
      "squidly    1.75 087  Good       Top secret module  Wildernes\n",
      "geoffrey   1.50 088  Good      Elefant Hunt insert missing  Falling \n",
      "damage  MARVELPhile\n",
      "geoffrey   1.50 089  Good      Creature catalog missing  Shields  sci fi\n",
      "TN.DE7     1.5  094  Good       Ranger changes  Creature catalog II \n",
      "detached but included\n",
      "geoffrey   1.50 095  Fair      Cover missing  Into the Forgotten Realms \n",
      "module, detached but included\n",
      "ykcheu     2.75 098  Fair       9th anniversary  Dragons  mutant manual\n",
      "UCCXKVB    1.75 099  Poor      Cover Missing  Treasure trove II, some \n",
      "pictures cut out\n",
      "thedm      2.50 100  Good      poster missing  city beyond the gate \n",
      "module detached but included  raised dragon texture on cover\n",
      "geoffrey   1.50 101  Fair      Cover missing  creature catalog III \n",
      "detached but included\n",
      "geoffrey   1.50 102  Fair      Cover missing  Valley of earth mother \n",
      "middle level module detached but included\n",
      "geoffrey   1.50 103  Fair      Cover loosly attached  Unearth arcana \n",
      "update missing  Future of ADD  Centaur papers\n",
      "geoffrey   1.50 103  Fair      Unearth arcana update missing \n",
      "geoffrey   1.50 104  FairVF    Marvel module  thieves  cover detached \n",
      "but included                                                                 \n",
      "geoffrey   1.50 105  Fair       ADD module  invisibility  cover  back \n",
      "cover detached but included      \n",
      "UCCXKVB    2.00 106  Fair       Cover 12 on  variations of paladins  \n",
      "more skills 4 rangers\n",
      "ykcheu     2.75 106  GoodVG    Variations of paladins  more skills for \n",
      "rangers    \n",
      "arrow      2.50 107  Fair      Cover missing  Dragons of glory \n",
      "supplementquestionaire\n",
      "geoffrey   1.50 108  Good       Mutant manual II  environmental \n",
      "effects  cover taped reinforced\n",
      "thedm      2.00 108  Very Good  Mutant manual II  environmental effects \n",
      "ykcheu     2.25 109  Good       Customizing DD classes  Agent 13 poster\n",
      "geoffrey   1.50 109  Very Fair  Customizing DD classes  Agent 13 poster \n",
      "missing\n",
      "geoffrey   1.50 110  Very Good  House on the frozen lands module 10th anniv\n",
      "squidly    1.75 110  Very Good  House on the frozen lands module 10th anniv\n",
      "geoffrey   1.50 111  Good       Murder Mystery ADD module\n",
      "ykcheu     2.50 112  Very Good  Ultimate Article Index  Mesozoic monsters\n",
      "mayla      2.50 114  Very Fair  Elven Cavalier  remorhaz  Witch NPC class\n",
      "ykcheu     1.75 115  Good       Theives  harpies  snakes\n",
      "squidly    1.75 116  GoodVG    3D ship cardboard insert  wild \n",
      "animals  dr who\n",
      "UCCXKVB    2.50 117  GoodVG    Dice odds  creative campaigns  sage \n",
      "advice  bazaar\n",
      "geoffrey   1.50 118  Good       TournamentsCompetitions  Nibars keep game\n",
      "UCCXKVB    2.00 120  VGEX      April fools issue\n",
      "UCCXKVB    2.00 121  Excellent  Oriental adventures  cardboard castle \n",
      "insert\n",
      "geoffrey   1.50 122  Excellent  11th aniversary  African beasts  druids\n",
      "UCCXKVB    2.50 123  Very Good  Magic and wizardry\n",
      "thedm      2.25 123  Very Good  Magic and wizardry\n",
      "arrow      2.5  124  Excellent  Aerial adventures  2nd edition ?aire\n",
      "UCCXKVB    2.25 124  Excellent  Aerial adventures  2nd edition ?aire\n",
      "geoffrey   1.50 125  Very Good  ClayORama!\n",
      "ArchiveName recautospart2\n",
      "\n",
      "Automotive Mailing Lists Electronic Mail, that is\n",
      "\n",
      "last updated 31793 new lotus, exotic cars list subscription info\n",
      "   added Portland, OR motorsports list, Corvair list, Triumph TR8 list  rpw\n",
      "\n",
      "There are a number of electronic mailing lists on the network devoted to\n",
      "various special automotive topics.\n",
      "\n",
      "Extractive Summary for Topic 3:\n",
      "Beans, coca, instant coffee, parsley, rhubarb, spinach and tea.\n",
      "That nothing be read in the Church under the nmae of Divine Scripture,\n",
      "except the canonical Scriptures, and the canoncial Scriptures are \n",
      "Genesis, Exodus, Leviticus, Numbers, Deuteronomy, Joshua, Judges, Ruth,\n",
      "Four books of Kingdoms being 12 Samuel and 12 Kings, Two books of\n",
      "Paralpomenon being 12 Chronicles, Job, the Psalter of David, the Five\n",
      "books of Solomon being Proverbs, Ecclesiastes, Song of Songs, Wisdom of\n",
      "Solomon, and misatributed to him the Wisdom of Jesus son of Sirach,\n",
      "The books of the Twelve Minor Prophets, Isaiah, Jeremiah being\n",
      "Jeremiah, the Lamentations, Baruch, and the Letter, all of which were\n",
      "formerly counted as one, Ezekiel, Daniel, Tobit, Judith, Esther, Two\n",
      "books of Ezra being Ezra and Nehemiah, Two books of Maccabees.\n",
      "\n",
      "Extractive Summary for Topic 4:\n",
      "Sundogs                         975\t1037.1\t0.4\t166\n",
      "167.\n",
      "Builtin T\n",
      "        Includes one 50 Ohm terminator\n",
      "        2 shipping will be added to your bid\n",
      "\n",
      "5 Super Nintendo Super Controller\n",
      "        Auto Repeated Fire\n",
      "        HandsFree Continuous Fire\n",
      "        Slow Motion accomplished by automatically pausingunpausing\n",
      "\tAdd to a onecontroller console to allow two players\n",
      "        2 shipping will be added to your bid\n",
      "\n",
      "\n",
      "Be sure to include a  SHIPPING ADDRESS with ALL bids.\n",
      "\n",
      "Extractive Summary for Topic 5:\n",
      "INFINITY RS6903 \n",
      "6x9 ThreeWay\n",
      "Freq.\n",
      "Mr. England wrote the \n",
      "following as part of a book review section in This Peoples magazine Spring \n",
      "1993 edition\n",
      "\n",
      "  I conclude with a little sermon because I believe we will not be a Mormon\n",
      "  or humanfamily until we can get over labeling and rejecting each other \n",
      "  with terms like feminist or patriarchal, liberal or conservative Christian \n",
      "  or nonChristian  Hal 8.\n",
      "\n",
      "Extractive Summary for Topic 6:\n",
      "on the likes of A.M.,\n",
      "  Jaguar, or sob Lotus.\n",
      "I called ElekTek one of the distributors, and\n",
      "they wanted to charge me 16 for cable, and gave only 1 year warranty...\n",
      "\n",
      "USR Sportster for the Mac is also highly but not as highly recommended\n",
      "its only 250 from ClubMac, and if you are willing to roll your own cable\n",
      "and dont care about the FAXstf software, you can get the generic model\n",
      "from PC outlets for 190.\n",
      "\n",
      "Extractive Summary for Topic 7:\n",
      "Honorable mentions to Majestic Marty and Warren Rychel.\n",
      "Eventually,\n",
      "he worked out a deal and on Dec 4, 1992 he met with Koptev, who heads the \n",
      "Russian space program, to sign the deal.\n",
      "\n",
      "Extractive Summary for Topic 8:\n",
      "?WWIZM?W,3Q,3Q,3Q,3Q,3Q,\n",
      "M3Q,B8FB8E,3Q,3Q,3Q,3Q,3PPPTQ,VG97Q30T\n",
      "MOM4PL!D90NRIZM?1T1TEI6EI49FQQ,3Q\n",
      "MQ9F9F9FPLI4WWIZGHJBHJBHJNUYNRIZGIZGHJBHJBIZM?\n",
      "Warning Representation size 8 must match superclasss to override background\n",
      "Warning translation table syntax error Modifier or  expected\n",
      "Warning ... found while parsing pj?P\n",
      "X Error of failed request  BadColor invalid Colormap parameter\n",
      "  Major opcode of failed request  85 XAllocNamedColor\n",
      "  Resource id in failed request  0x0\n",
      "  Serial number of failed request  18\n",
      "  Current serial number in output stream  18\n",
      "\n",
      "Other than this, all the other core X stuff seems to be working OK.  Any\n",
      "hintstips appreciated, patches would be primo.\n",
      "\n",
      "Extractive Summary for Topic 9:\n",
      "AIU  Alliance Israelite Universelle, Paris.\n",
      "This agreement operated on a number of fronts \n",
      "helping Jews to leave the country, breaking the ring of the\n",
      "boycott, exporting German goods in large quantities to Palestine,\n",
      "and last but not least, enabling the regime to be seen as humane\n",
      "and reasonable even towards its avowed enemies, the Jews.\n",
      "\n",
      "Extractive Summary for Topic 10:\n",
      "\n",
      "Yes, a point welltaken ... however, even in areas that finally got\n",
      "some games, theres something nagging in the back of your skull when\n",
      "the network that has the national rights in its pocket says on its\n",
      "sports news, Theres an awesome overtime going on in Quebec City,\n",
      "and well try to get you an update through the show ... when you\n",
      "know that its on a satellites feedhorn somewhere up there ...\n",
      "\n",
      "\n",
      "From todays Times, ABC got great ratings in Chicago and St. Louis a\n",
      "4.2, and the KingsFlames got a 2.9 on the West Coast, but only a 2.2\n",
      "in metro New York i.e., the Devils squandered their newfound support\n",
      "from a year ago when they played the Rangers .\n",
      "The \n",
      "rotation should be Sutcliffe, Mussina, McDonald, Rhodes, ?\n",
      "\n",
      "Extractive Summary for Topic 11:\n",
      "Atoms, trees, electrons are all independently observable and\n",
      "verifiable.\n",
      "I try to edit this newsgroup and feed it to one of the local elementary schools,\n",
      "they have a group of students that just love baseball and are learning to use\n",
      "computers, but Im telling you, its gotten to the point that I dont even edit\n",
      "the files anymore, just read them and throw out the trash...  And thanks to all\n",
      "you people that think its wonderful to include a swear word or two in your\n",
      "signature files, thats really nice...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
    "    text = re.sub(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', '', text)  # Remove phone numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    \"\"\"Tokenize and clean a list of documents.\"\"\"\n",
    "    all_sentences = []\n",
    "    all_tokenized_sentences = []\n",
    "    \n",
    "    for text in texts:\n",
    "        sentences = sent_tokenize(text)  # Split into sentences\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())  # Lowercase & tokenize\n",
    "            words = [w for w in words if w not in stop_words and w not in string.punctuation]  # Remove stopwords & punctuation\n",
    "            if len(words) >= 5:  # Filter out short sentences\n",
    "                all_tokenized_sentences.append(words)  \n",
    "                all_sentences.append(sentence)  \n",
    "\n",
    "    return all_tokenized_sentences, all_sentences\n",
    "\n",
    "def train_lda(documents, num_topics=10):\n",
    "    \"\"\"Train an LDA model using Gensim.\"\"\"\n",
    "    dictionary = Dictionary(documents)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=40, random_state=42)\n",
    "    return lda_model, dictionary, bow_corpus\n",
    "\n",
    "def get_topic_distribution(lda_model, bow):\n",
    "    \"\"\"Get topic distribution for a document or summary.\"\"\"\n",
    "    topic_probs = lda_model.get_document_topics(bow, minimum_probability=1e-10)\n",
    "    topic_dict = {k: v for k, v in topic_probs}  \n",
    "    return np.array([topic_dict.get(k, 1e-10) for k in range(lda_model.num_topics)])\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence between two probability distributions.\"\"\"\n",
    "    return entropy(p, q)\n",
    "\n",
    "def kl_sum_per_topic(sentences, tokenized_sentences, lda_model, dictionary, summary_length=3):\n",
    "    \"\"\"Generate separate extractive summaries for each topic.\"\"\"\n",
    "    \n",
    "    # Step 1: Get topic distribution for each sentence\n",
    "    sentence_topic_dists = {}\n",
    "    for i, sentence in enumerate(tokenized_sentences):\n",
    "        bow = dictionary.doc2bow(sentence)\n",
    "        if len(bow) > 0:  # Avoid empty docs\n",
    "            sentence_topic_dists[sentences[i]] = get_topic_distribution(lda_model, bow)\n",
    "\n",
    "    topic_summaries = {}\n",
    "\n",
    "    # Step 2: Extract summaries per topic\n",
    "    for topic_id in range(lda_model.num_topics):\n",
    "        PD = np.zeros(lda_model.num_topics)  \n",
    "        num_sentences = 0  \n",
    "\n",
    "        # Compute PD: The topic distribution of all sentences that strongly belong to this topic\n",
    "        for sent, dist in sentence_topic_dists.items():\n",
    "            if dist[topic_id] > 0.25:  # Select sentences with meaningful weight for this topic\n",
    "                PD += dist  \n",
    "                num_sentences += 1  \n",
    "\n",
    "        if num_sentences == 0:\n",
    "            continue  \n",
    "\n",
    "        PD /= num_sentences  # Normalize PD\n",
    "\n",
    "        selected_sentences = []\n",
    "        PS = np.ones_like(PD) * 1e-10  # Initialize uniform topic distribution\n",
    "\n",
    "        while len(selected_sentences) < summary_length:\n",
    "            best_sentence = None\n",
    "            best_kl_score = float(\"inf\")\n",
    "\n",
    "            for sentence, topic_dist in sentence_topic_dists.items():\n",
    "                if sentence in selected_sentences or topic_dist[topic_id] < 0.1:\n",
    "                    continue  \n",
    "\n",
    "                new_PS = (PS * len(selected_sentences) + topic_dist) / (len(selected_sentences) + 1)\n",
    "                kl_score = kl_divergence(PD, new_PS)\n",
    "\n",
    "                if kl_score < best_kl_score:\n",
    "                    best_kl_score = kl_score\n",
    "                    best_sentence = sentence\n",
    "\n",
    "            if best_sentence:\n",
    "                selected_sentences.append(best_sentence)\n",
    "                PS = (PS * (len(selected_sentences) - 1) + sentence_topic_dists[best_sentence]) / len(selected_sentences)\n",
    "\n",
    "        topic_summaries[topic_id] = selected_sentences\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "# Load 20 Newsgroups dataset\n",
    "documents = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "documents = documents.data  # Use a subset for faster training\n",
    "documents = [clean_text(doc) for doc in documents]\n",
    "\n",
    "# Preprocess dataset (get tokenized sentences & original sentences)\n",
    "tokenized_sentences, original_sentences = preprocess_texts(documents)\n",
    "\n",
    "# Train LDA on the dataset\n",
    "lda_model, dictionary, bow_corpus = train_lda(tokenized_sentences, num_topics=12)\n",
    "\n",
    "# Generate KL-Sum extractive summaries for each topic\n",
    "topic_summaries = kl_sum_per_topic(original_sentences, tokenized_sentences, lda_model, dictionary, summary_length=2)\n",
    "\n",
    "# Print summaries per topic\n",
    "for topic, summary in topic_summaries.items():\n",
    "    print(f\"\\nExtractive Summary for Topic {topic}:\")\n",
    "    print(\"\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extractive Summary for Topic 0:\n",
      "It was drawn by three pairs of black horses .\n",
      "Banners welcoming him back were draped around the arena .\n",
      "\n",
      "Extractive Summary for Topic 1:\n",
      "Challenging them will be Uta Pippig , Wanda Panfil and Kim Jones.\n",
      "Korologos asked the exhibitor ,  Why are you ruining this perfectly legitimate rifle ?\n",
      "\n",
      "Extractive Summary for Topic 2:\n",
      "He studiously ignores other major differences between 1791 and today .\n",
      "Barber Conable is an outstanding person ,  he says .\n",
      "\n",
      "Extractive Summary for Topic 3:\n",
      "Rosendo Herrera , flight engineer  Sgt.\n",
      "Carpio Villarreal , flight engineer  Sgt.\n",
      "\n",
      "Extractive Summary for Topic 4:\n",
      "Nevertheless , Venetia originally was to have employed 870 and it now has 764 .\n",
      "Prediction hinges on spotting anomalous phenomena , or precursors .\n",
      "\n",
      "Extractive Summary for Topic 5:\n",
      "In Rotterdam on Sunday , Belayneh Dinsamo won in 20839 .\n",
      "They are a conservative bunch , not given to Sundaysupplement scare stories .\n",
      "\n",
      "Extractive Summary for Topic 6:\n",
      "Then we said ,  Oh no , this is nice Johnny Kelley .\n",
      "We are defending the officers being in that apartment ,  Mills said .\n",
      "\n",
      "Extractive Summary for Topic 7:\n",
      "Jade , aloe vera , phormium and evergreen currant are among the highmoisturecontent plants characteristic of this zone .\n",
      "Under the Carter administration , the agency had zealously promoted numerical goals and timetables .\n",
      "\n",
      "Extractive Summary for Topic 8:\n",
      "Bridgman scoops up a shovelful of gravel , lugs it to the water s edge and dumps it in .\n",
      "In San Jose , CalOSHA can be reached at LRB408RRB 4527288 .\n",
      "\n",
      "Extractive Summary for Topic 9:\n",
      "U.S. 395 between Milford and Janesville was closed .\n",
      "A brochure available at the garden aids your exploration .\n",
      "\n",
      "Extractive Summary for Topic 10:\n",
      "These are  Yusuf Sha  ban , Bassam Muhammad  Attiyah , and Yusuf  Udwani , codenamed Salim Mahyub .\n",
      "It was delivered in May 1982 to nowdefunct Frontier Airlines .\n",
      "\n",
      "Extractive Summary for Topic 11:\n",
      ", Walkdon , Worslay , Manchester M28 5DQ , United Kingdom .\n",
      "The second point of the document of the Central Committee meeting presided over by  Feliciano  highlights the agreements of the Working Meeting of the Shining Path leadership held in August 1993 , almost one year after Guzman was arrested , and during which the implementation of the agreements of the Third Plenum held in March 1992 was discussed .\n",
      "\n",
      "Extractive Summary for Topic 12:\n",
      "What kind of development legacy will Mr Preston inherit ?\n",
      "He will listen to everyone s side ,  a Pickett aide says .\n",
      "\n",
      "Extractive Summary for Topic 13:\n",
      "Essentially , we re stuck with another paradox ,  he adds .\n",
      "Gases such as carbon monoxide , however , are not filtered out .\n",
      "\n",
      "Extractive Summary for Topic 14:\n",
      "The expectation was that every man would have his own firearms .\n",
      "India s chief election commissioner , T.N.\n",
      "\n",
      "Extractive Summary for Topic 15:\n",
      "Qualifications  The Constitution sets no qualifications for justices .\n",
      "Bauer House of Lords , Westminster SW1\n",
      "\n",
      "Extractive Summary for Topic 16:\n",
      "Permanent restriction of tanker traffic to daylight hours .\n",
      "In San Jose , CalOSHA can be reached at LRB408RRB 4527288 .\n",
      "\n",
      "Extractive Summary for Topic 17:\n",
      "Mr Jontz disagrees , reckoning there are now more Republicans than Democrats on the fence .\n",
      "Keleke Metaferia won in 21028 , and Dereje Nedi was second in 21036 .\n",
      "\n",
      "Extractive Summary for Topic 18:\n",
      "from Holy Cross College , 1971  J.D.\n",
      "Tunnelers are expected to walk through and greet each other with handshakes in a few weeks .\n",
      "\n",
      "Extractive Summary for Topic 19:\n",
      "This solution is more appealing than a mere cessation of benefits .\n",
      "Both Mr Kucan and Mr Drnovsek , who headed the old revolving Yugoslav state presidency for a year before negotiating the exodus of the federal army from Slovenia in July 1991 , criticise the failure of the west to intervene more forcibly to stop at an early stage what Mr Kucan calls  the war of aggression waged by Serbia .\n",
      "\n",
      "Extractive Summary for Topic 20:\n",
      "What kind of development legacy will Mr Preston inherit ?\n",
      "His time works out to 500.8 mile splits .\n",
      "\n",
      "Extractive Summary for Topic 21:\n",
      "Precursors are often only recognised as such after a large earthquake .\n",
      "Nor is the booming field of microlending without its detractors  or its controversies .\n",
      "\n",
      "Extractive Summary for Topic 22:\n",
      "Suddenly , xeriscaping  landscaping with droughttolerant natives  is becoming commonplace and fashionable .\n",
      "The buzz at the wildfire nerve center intensified dramatically .\n",
      "\n",
      "Extractive Summary for Topic 23:\n",
      "They shouldered peagreen SIG assault rifles .\n",
      "Budapest offers a halfmarathon and a minimarathon along the Danube.\n",
      "\n",
      "Extractive Summary for Topic 24:\n",
      "But others said the LAPD had behaved with restraint .\n",
      "Some critics cynically attribute his ideological metamorphosis to opportunism.\n",
      "\n",
      "Extractive Summary for Topic 25:\n",
      "Here worn out equipment will be stockpiled and sold off when the mine closes .\n",
      "The attack on DeConcini was stirred up by the National Rifle Assn.\n",
      "\n",
      "Extractive Summary for Topic 26:\n",
      "That s a semiautomatic weapon , and I do not plan to surrender it ,  he proclaimed , banging the lectern .\n",
      "Kim Jones of Spokane , Wash. , was third in 22934 .\n",
      "\n",
      "Extractive Summary for Topic 27:\n",
      "I realized that the landscaping had a relationship to the amount of devastation .\n",
      "We are so excited ,  Pamela Dei Rossi said .\n",
      "\n",
      "Extractive Summary for Topic 28:\n",
      "But  transmission is dose dependent ,  Collinge says .\n",
      "Most right thinkers take comfort in that funny stuff about the militia .\n",
      "\n",
      "Extractive Summary for Topic 29:\n",
      "Each resembles a gnawing worm , some threestories high , with a spinning drill of hundreds of blades .\n",
      "It was a revival ,  said Texas Congressman JJ Pickle .\n"
     ]
    }
   ],
   "source": [
    "duc_dataset = load_dataset(\"midas/duc2001\", \"raw\")\n",
    "duc_documents = duc_dataset['test']['document']\n",
    "duc_documents = [\" \".join(doc) for doc in duc_documents]\n",
    "duc_documents = [clean_text(doc) for doc in duc_documents]\n",
    "\n",
    "tokenized_sentences_duc, original_sentences_duc = preprocess_texts(duc_documents)\n",
    "\n",
    "# Train LDA on the dataset\n",
    "lda_model_duc, dictionary_duc, bow_corpus_duc = train_lda(tokenized_sentences_duc, num_topics=30)\n",
    "\n",
    "# Generate KL-Sum extractive summaries for each topic\n",
    "topic_summaries_duc = kl_sum_per_topic(original_sentences_duc, tokenized_sentences_duc, lda_model_duc, dictionary_duc, summary_length=2)\n",
    "\n",
    "# Print summaries per topic\n",
    "for topic, summary in topic_summaries_duc.items():\n",
    "    print(f\"\\nExtractive Summary for Topic {topic}:\")\n",
    "    print(\"\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duc_dataset = load_dataset(\"midas/duc2001\", \"raw\")\n",
    "duc_documents = duc_dataset['test']['document']\n",
    "duc_documents = [\" \".join(doc) for doc in duc_documents]\n",
    "duc_documents = [clean_text(doc) for doc in duc_documents]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
