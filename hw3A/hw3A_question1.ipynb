{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml, fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import codecs\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading t10k-images-idx3-ubyte\n",
      "Reading t10k-labels-idx1-ubyte\n",
      "Reading train-images-idx3-ubyte\n",
      "Reading train-labels-idx1-ubyte\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "path_to_data = os.environ.get(\"MNIST_DATAPATH\")\n",
    "mnist_files = os.listdir(path_to_data)\n",
    "mnist_files = [x for x in mnist_files if x.endswith(\"ubyte\")]\n",
    "\n",
    "\n",
    "def convert_to_int(byte):\n",
    "    integer = int(codecs.encode(byte, 'hex'), 16)\n",
    "    return integer\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "for file in mnist_files:\n",
    "    print(\"Reading\", file)\n",
    "    with open(path_to_data + file, \"rb\") as f:\n",
    "        data = f.read()\n",
    "        type_of_data = convert_to_int(data[:4])\n",
    "        length = convert_to_int(data[4:8])\n",
    "        if type_of_data == 2051:\n",
    "            category = \"images\"\n",
    "            number_of_rows = convert_to_int(data[8:12])\n",
    "            number_of_columns = convert_to_int(data[12:16])\n",
    "            parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "            parsed = parsed.reshape(length, number_of_rows, number_of_columns)\n",
    "        if type_of_data == 2049:\n",
    "            category = \"labels\"\n",
    "            parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "            parsed = parsed.reshape(length)\n",
    "        if length == 60000:\n",
    "            set = \"train\"\n",
    "        if length == 10000:\n",
    "            set = \"test\"\n",
    "        dataset[set + '_' + category] = parsed\n",
    "\n",
    "print(dataset[\"train_images\"][0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = dataset['train_images']\n",
    "train_labels = dataset['train_labels']\n",
    "train_images_flattened = train_images.reshape(60000, -1)\n",
    "train_images_flattened = train_images_flattened / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         7850     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.30259D+00    |proj g|=  6.37317D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.62748D-01    |proj g|=  2.53447D-03\n",
      "\n",
      "At iterate  100    f=  2.42439D-01    |proj g|=  9.02920D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 7850    100    107      1     0     0   9.029D-04   2.424D-01\n",
      "  F =  0.24243853456327233     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Logistic Regression accuracy on MNIST dataset: 0.8699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=100, multi_class=\"multinomial\", verbose=1, n_jobs=-1)\n",
    "log_reg.fit(train_images_flattened, train_labels)\n",
    "\n",
    "y_pred_log = log_reg.predict(dataset['test_images'].reshape(10000, -1))\n",
    "log_accuracy = accuracy_score(dataset['test_labels'], y_pred_log)\n",
    "print(f'Logistic Regression accuracy on MNIST dataset: {log_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 important features for Logistic Regression (pixel indices): [249 465 221  97 459 387 377 348 369 621 434 708 742 332 388 376 404 370\n",
      " 343 248 473 360 358 501 277 359 333 276 305 304]\n"
     ]
    }
   ],
   "source": [
    "top_features_log = np.argsort(np.abs(log_reg.coef_).max(axis=-0))[-30:]\n",
    "print(f\"Top 30 important features for Logistic Regression (pixel indices):\", top_features_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy on MNSIT dataset: 0.8283\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=10, random_state=97)\n",
    "dt_model.fit(train_images_flattened, train_labels)\n",
    "\n",
    "y_pred_dt = dt_model.predict(dataset['test_images'].reshape(10000, -1))\n",
    "dt_accuracy = accuracy_score(dataset['test_labels'], y_pred_dt)\n",
    "print(f\"Decision Tree accuracy on MNSIT dataset: {dt_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique features used in Decision Tree: 328\n",
      "Feature indices: [ 67  70  73  92  95  96  97  98 100 101 102 103 123 125 126 127 128 129\n",
      " 132 133 147 148 149 150 152 153 154 155 156 158 159 161 162 164 172 176\n",
      " 177 178 179 180 182 183 185 186 187 188 190 203 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 220 228 230 232 234 235 236 237 238 239 240\n",
      " 241 242 243 244 247 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 276 286 287 289 290 291 292 293 294 295 296 297 298 299 300 301 314 315\n",
      " 316 317 318 319 320 321 322 323 324 325 326 328 329 342 343 344 345 346\n",
      " 347 348 349 350 351 352 353 354 355 356 357 358 360 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 396 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 412 413 416 426 427 428 429 430 431 432 433\n",
      " 434 435 436 437 438 439 440 441 442 443 453 454 455 456 457 458 459 460\n",
      " 461 462 463 464 465 466 467 468 469 470 473 482 483 484 485 486 487 488\n",
      " 489 490 491 492 493 494 496 497 498 500 510 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 537 538 539 541 542 543 544 545 546\n",
      " 547 550 553 554 555 564 565 568 570 571 572 573 574 575 577 578 579 580\n",
      " 581 582 594 595 596 597 598 599 600 601 602 603 606 608 610 622 623 624\n",
      " 625 626 627 628 630 632 633 634 635 636 640 652 653 654 655 656 657 658\n",
      " 659 660 662 665 678 679 680 682 683 684 685 686 687 688 705 707 711 712\n",
      " 716 717 719 738]\n",
      "Top 30 important features for Decision Tree (pixel indices):  [300 514  95 297 296 267 348 658 381 101 271 155 153 402 655 354 290 486\n",
      " 484  98 156 405 234 346 211 430 350 568 435 489]\n"
     ]
    }
   ],
   "source": [
    "used_features = np.where(dt_model.feature_importances_ > 0)[0]\n",
    "\n",
    "print(f\"Number of unique features used in Decision Tree: {len(used_features)}\")\n",
    "print(\"Feature indices:\", used_features)\n",
    "\n",
    "top_features_dt = np.argsort(dt_model.feature_importances_)[-30:]\n",
    "print(f\"Top 30 important features for Decision Tree (pixel indices): \", top_features_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spambase Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 4601\n",
      "Label distribution: class\n",
      "0    2788\n",
      "1    1813\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "spambase = fetch_openml(name=\"Spambase\", version=1, parser=\"pandas\")\n",
    "features_sb = spambase.data\n",
    "labels_sb = spambase.target.astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {len(labels_sb)}\")\n",
    "print(\"Label distribution:\", labels_sb.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (3680, 57)\n",
      "Test set shape: (921, 57)\n"
     ]
    }
   ],
   "source": [
    "features_sb_train, features_sb_test, labels_sb_train, labels_sb_test = train_test_split(features_sb, labels_sb, test_size=0.2, random_state=97)\n",
    "print(f\"Training set shape: {features_sb_train.shape}\")\n",
    "print(f\"Test set shape: {features_sb_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           58     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  4.30524D+01\n",
      "\n",
      "At iterate   50    f=  2.90396D-01    |proj g|=  3.12759D+00\n",
      "\n",
      "At iterate  100    f=  2.43916D-01    |proj g|=  1.68112D-01\n",
      "\n",
      "At iterate  150    f=  2.33950D-01    |proj g|=  5.36825D-01\n",
      "\n",
      "At iterate  200    f=  2.17091D-01    |proj g|=  1.16747D-01\n",
      "\n",
      "At iterate  250    f=  2.13982D-01    |proj g|=  6.24229D-03\n",
      "\n",
      "At iterate  300    f=  2.13660D-01    |proj g|=  1.51554D-01\n",
      "\n",
      "At iterate  350    f=  2.12783D-01    |proj g|=  1.33006D-01\n",
      "\n",
      "At iterate  400    f=  2.11331D-01    |proj g|=  9.97603D-02\n",
      "\n",
      "At iterate  450    f=  2.10776D-01    |proj g|=  1.95436D-02\n",
      "\n",
      "At iterate  500    f=  2.08648D-01    |proj g|=  1.39754D-02\n",
      "\n",
      "At iterate  550    f=  2.08364D-01    |proj g|=  4.87844D-01\n",
      "\n",
      "At iterate  600    f=  2.08112D-01    |proj g|=  1.86070D-01\n",
      "\n",
      "At iterate  650    f=  2.07732D-01    |proj g|=  7.17498D-02\n",
      "\n",
      "At iterate  700    f=  2.07568D-01    |proj g|=  3.82326D-02\n",
      "\n",
      "At iterate  750    f=  2.07338D-01    |proj g|=  6.24758D-01\n",
      "\n",
      "At iterate  800    f=  2.07231D-01    |proj g|=  3.32411D-02\n",
      "\n",
      "At iterate  850    f=  2.07051D-01    |proj g|=  1.40896D-02\n",
      "\n",
      "At iterate  900    f=  2.06838D-01    |proj g|=  8.78566D-03\n",
      "\n",
      "At iterate  950    f=  2.06795D-01    |proj g|=  4.60929D-02\n",
      "\n",
      "At iterate 1000    f=  2.06659D-01    |proj g|=  7.05907D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   58   1000   1202      1     0     0   7.059D-03   2.067D-01\n",
      "  F =  0.20665894246169272     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on Spambase: 0.9121\n"
     ]
    }
   ],
   "source": [
    "log_reg_sb = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=1000, verbose=1, n_jobs=-1)\n",
    "log_reg_sb.fit(features_sb_train, labels_sb_train)\n",
    "y_pred_log_sb = log_reg_sb.predict(features_sb_test)\n",
    "\n",
    "log_accuracy_sb = accuracy_score(labels_sb_test, y_pred_log_sb)\n",
    "print(f'Logistic Regression Accuracy on Spambase: {log_accuracy_sb:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 important features for Logistic Regression (feature indices): [42 29 16 14  3  5 35 46 44 30 32 38 53  7 25 15 19 48 34 28 47 40 43 45\n",
      " 24 41  6 22 52 26]\n"
     ]
    }
   ],
   "source": [
    "top_features_log_sb = np.argsort(np.abs(log_reg_sb.coef_).flatten())[-30:]\n",
    "print(\"Top 30 important features for Logistic Regression (feature indices):\", top_features_log_sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on Spambase: 0.9077\n"
     ]
    }
   ],
   "source": [
    "dt_model_sb = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "dt_model_sb.fit(features_sb_train, labels_sb_train)\n",
    "\n",
    "y_pred_dt_sb = dt_model_sb.predict(features_sb_test)\n",
    "\n",
    "dt_accuracy_sb = accuracy_score(y_pred_dt_sb, labels_sb_test)\n",
    "print(f'Decision Tree Accuracy on Spambase: {dt_accuracy_sb:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 important features for Decision Tree (feature indices): [32 29 53 25 12 10  9 22  7 11 44 18 27 41 16 36 56 49  4 54 20 45 26 23\n",
      " 52 24 15 55  6 51]\n"
     ]
    }
   ],
   "source": [
    "top_features_dt_sb = np.argsort(dt_model_sb.feature_importances_)[-30:]\n",
    "print(\"Top 30 important features for Decision Tree (feature indices):\", top_features_dt_sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (2712, 35163)\n",
      "Test set shape: (678, 35163)\n"
     ]
    }
   ],
   "source": [
    "categories = [\"alt.atheism\", \"sci.med\", \"sci.electronics\", \"comp.graphics\", \"talk.politics.guns\", \"sci.crypt\"]\n",
    "newsgroups_train = fetch_20newsgroups(categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "X = newsgroups_train.data\n",
    "y = newsgroups_train.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=97)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# newsgroups_test = fetch_20newsgroups(subset=\"test\", categories=categories, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "# X_test = newsgroups_test.data\n",
    "# y_test = newsgroups_test.target\n",
    "\n",
    "# vectorizer_1 = TfidfVectorizer()\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy on 20 Newsgroups: 0.8304\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=1000, multi_class=\"multinomial\", verbose=0, n_jobs=-1)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "y_prediction_logistic_reg = logistic_regression.predict(X_test)\n",
    "logistic_regression_accuracy = accuracy_score(y_test, y_prediction_logistic_reg)\n",
    "print(f'Logistic Regression Accuracy on 20 Newsgroups: {logistic_regression_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 important words for Logistic Regression: ['algorithm' 'he' '3d' 'files' 'use' 'circuit' 'keys' 'they' 'religion'\n",
      " 'thanks' 'file' 'is' 'people' 'you' 'nsa' 'chip' 'msg' 'weapons' 'image'\n",
      " 'of' 'guns' 'that' 'god' 'the' 'clipper' 'encryption' 'government' 'key'\n",
      " 'graphics' 'gun']\n"
     ]
    }
   ],
   "source": [
    "top_30_features = np.argsort(np.abs(logistic_regression.coef_).sum(axis=0))[-30:]\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "top_words_log = feature_names[top_30_features]\n",
    "\n",
    "print(\"Top 30 important words for Logistic Regression:\", top_words_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy on 20 Newsgroups: 0.5855\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_prediction_decision_tree = decision_tree.predict(X_test)\n",
    "\n",
    "dt_acc = accuracy_score(y_test, y_prediction_decision_tree)\n",
    "print(f'Decision Tree Accuracy on 20 Newsgroups: {dt_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 important words for Decision Tree: ['of' 'treatment' 'electronics' 'morality' 'image' 'it' 'is' 'in' 'and'\n",
      " 'this' 'that' 'power' 'thanks' 'files' 'msg' 'fbi' 'disease' 'religion'\n",
      " 'weapons' 'to' 'government' 'doctor' 'clipper' 'circuit' 'gordon' 'god'\n",
      " 'key' 'graphics' 'gun' 'encryption']\n"
     ]
    }
   ],
   "source": [
    "top_features_decision_tree = np.argsort(decision_tree.feature_importances_)[-30:]\n",
    "\n",
    "top_words_decision_tree = feature_names[top_features_decision_tree]\n",
    "\n",
    "print(\"Top 30 important words for Decision Tree:\", top_words_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy (max_depth=5): 0.3702\n",
      "Decision Tree Accuracy (max_depth=15): 0.5855\n"
     ]
    }
   ],
   "source": [
    "dt_model_small = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_model_small.fit(X_train, y_train)\n",
    "\n",
    "dt_model_large = DecisionTreeClassifier(max_depth=200, random_state=42)\n",
    "dt_model_large.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt_small = dt_model_small.predict(X_test)\n",
    "y_pred_dt_large = dt_model_large.predict(X_test)\n",
    "\n",
    "acc_small = accuracy_score(y_test, y_pred_dt_small)\n",
    "acc_large = accuracy_score(y_test, y_pred_dt_large)\n",
    "\n",
    "print(f'Decision Tree Accuracy (max_depth=5): {acc_small:.4f}')\n",
    "print(f'Decision Tree Accuracy (max_depth=15): {acc_large:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
