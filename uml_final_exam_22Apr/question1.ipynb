{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08ec4c1",
   "metadata": {},
   "source": [
    "I have loaded the data. Separated the data and labels.\n",
    "Then I do pre processing with gaussian filter and StandardScaler.\n",
    "For processing, I do PCA upto 100 dim, and then UMAP to 20 dims.\n",
    "For clustering, I do DBSCAN to remove noise. Then I do KMeans with 10 components.\n",
    "Then I pass the clustered labels and true labels to the evaluate function for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74c7de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages (from umap-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages (from umap-learn) (1.5.2)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Downloading numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages (from umap-learn) (4.66.5)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Downloading llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/ajeyk/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: llvmlite, numba, pynndescent, umap-learn\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0 pynndescent-0.5.13 umap-learn-0.5.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16559f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import umap\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c3382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape:  (8358,)\n",
      "Data shape:  (8358, 784)\n",
      "Data shape after PCA:  (8358, 100)\n",
      "Data shape after t-SNE:  (8358, 20)\n",
      "DBSCAN labels: (array([0, 1, 2, 3, 4]), array([ 670, 3210,  731, 1586, 2161]))\n",
      "Number of noise points:  0\n",
      "Clustered labels shape:  (8358,)\n",
      "Max label:  9\n",
      "Min label:  0\n",
      "Cluster size distribution:\n",
      "Cluster 0: 504 samples\n",
      "Cluster 1: 488 samples\n",
      "Cluster 2: 1586 samples\n",
      "Cluster 3: 731 samples\n",
      "Cluster 4: 670 samples\n",
      "Cluster 5: 489 samples\n",
      "Cluster 6: 923 samples\n",
      "Cluster 7: 1033 samples\n",
      "Cluster 8: 1200 samples\n",
      "Cluster 9: 734 samples\n",
      "true 8358\n",
      "pred 8358\n",
      "Class Entropies: [0.045 0.014 0.146 0.095 0.261 0.176 0.03  0.241 0.328 0.157]\n",
      "Cluster Entropies: [0.114 0.225 0.055 0.066 0.016 0.135 0.032 0.201 0.096 0.505]\n",
      "Weighted average entropies: [0.132 0.132], (avg: 0.132)\n",
      "Confusion Matrix: \n",
      " [[   0    0    0    2  669    0    0    0    1    0]\n",
      " [   0    0 1578    0    0    2    0    0    0    0]\n",
      " [   0    0    3    0    0  482    0    2    3    0]\n",
      " [   1    9    0    0    0    1    0 1007    0    0]\n",
      " [   0    0    0    0    0    1  920    0    0   40]\n",
      " [   0    3    0    3    0    3    0   16 1188    0]\n",
      " [   0    0    1  726    0    0    0    0    1    0]\n",
      " [ 497    0    3    0    0    0    0    0    0   15]\n",
      " [   0  474    1    0    0    0    0    8    7    5]\n",
      " [   6    2    0    0    1    0    3    0    0  674]]\n",
      "Weighted Class Entropies:  0.13191156322584785\n",
      "Weighted Cluster Entropies:  0.13182070207813895\n",
      "Weighted Entropies:  [0.13191156 0.1318207 ]\n"
     ]
    }
   ],
   "source": [
    "def load_data(absolute_file_path):\n",
    "    with open(absolute_file_path, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].split(',')\n",
    "\n",
    "    labels = [-1] * len(data)\n",
    "    for i in range(len(data)):\n",
    "        labels[i] = data[i].pop(0)\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    data = data.astype(np.float32)\n",
    "    labels = labels.astype(np.int32)\n",
    "    print(\"Labels shape: \", labels.shape)\n",
    "    print(\"Data shape: \", data.shape)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def preprocess_images(data):\n",
    "    temp_data = data.reshape(-1, 28, 28)\n",
    "    step1_data = gaussian_filter(temp_data, sigma=0.75)\n",
    "    reshaped_data = step1_data.reshape(-1, 28 * 28)\n",
    "    # data = data.astype(np.float32)\n",
    "    scaler =  StandardScaler()\n",
    "    data = scaler.fit_transform(reshaped_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_data_tsne(data):\n",
    "    pca = PCA(n_components=100)\n",
    "    pca_reduced = pca.fit_transform(data)\n",
    "    print(\"Data shape after PCA: \", pca_reduced.shape)\n",
    "    umapped = umap.UMAP(n_components=20)\n",
    "    umap_reduced = umapped.fit_transform(pca_reduced)\n",
    "    print(\"Data shape after t-SNE: \", umap_reduced.shape)\n",
    "    return umap_reduced\n",
    "\n",
    "\n",
    "def perform_clustering(data):\n",
    "    # STep 1: Use DBSCAN to remove noise\n",
    "    db = DBSCAN(eps=3, min_samples=9).fit(data)\n",
    "    print(\"DBSCAN labels:\", np.unique(db.labels_, return_counts=True))\n",
    "\n",
    "    not_noise = db.labels_ != -1\n",
    "    print(\"Number of noise points: \", len(data[db.labels_ == -1]))\n",
    "    clean_data = data[not_noise]\n",
    "    kmeans = KMeans(n_clusters=10, n_init='auto')\n",
    "    kmeans.fit(clean_data)\n",
    "    clustered_labels = kmeans.labels_\n",
    "    print(\"Clustered labels shape: \", clustered_labels.shape)\n",
    "    print(\"Max label: \", np.max(clustered_labels))\n",
    "    print(\"Min label: \", np.min(clustered_labels))\n",
    "    return clustered_labels\n",
    "\n",
    "\n",
    "def evaluate(true_labels: np.ndarray, pred_labels: np.ndarray) -> tuple:\n",
    "    \"\"\"Entropy-based evaluation of a label assignment.\n",
    "\n",
    "    Parameters:\n",
    "    true_labels: the ground-truth class labels on the input data.\n",
    "    pred_labels: the predicted class labels on the input data.\n",
    "\n",
    "    Returns:\n",
    "    a tuple (CM, (cs_e, cr_e, we)) containing the confusion matrix `CM`, the class entropies `cs_e`,\n",
    "    the cluster entropies `cr_e`, and the averaged weighted entropies `we`.\n",
    "    \"\"\"\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    assert len(true_labels) == len(pred_labels), \"Label predictions don't match\"\n",
    "    print(\"true\", len(true_labels))\n",
    "    print(\"pred\", len(pred_labels))\n",
    "    ## Map the labels to index set {0, 1, ..., k - 1 }\n",
    "    t_classes, t_labels = np.unique(true_labels, return_inverse=True)\n",
    "    p_classes, p_labels = np.unique(pred_labels, return_inverse=True)\n",
    "    # assert np.all(np.isin(p_classes, t_classes)), \"Predicted class outside of labels given\"\n",
    "\n",
    "    ## Accumulate the counts\n",
    "    n_classes = len(t_classes)\n",
    "    CM = np.zeros(shape=(n_classes, n_classes), dtype=np.uint32)\n",
    "    ind = np.ravel_multi_index([t_labels, p_labels], CM.shape)\n",
    "    np.add.at(CM.ravel(), ind, 1)\n",
    "\n",
    "    ## Compute the entropy of the empirical row/column distributions\n",
    "    empirical_dist = lambda x: x / np.sum(x)\n",
    "    cluster_entropy = np.apply_along_axis(lambda x: entropy(empirical_dist(x), base=2), 0, CM)\n",
    "    class_entropy = np.apply_along_axis(lambda x: entropy(empirical_dist(x), base=2), 1, CM)\n",
    "\n",
    "    ## Average w/ count weights\n",
    "    w_cluster_entropy = np.sum(cluster_entropy * CM.sum(axis=0)) / len(true_labels)\n",
    "    w_class_entropy = np.sum(class_entropy * CM.sum(axis=1)) / len(true_labels)\n",
    "    w_entropies = np.array([w_class_entropy, w_cluster_entropy])\n",
    "\n",
    "    with np.printoptions(precision=3):\n",
    "        print(f\"Class Entropies: {class_entropy}\")\n",
    "        print(f\"Cluster Entropies: {cluster_entropy}\")\n",
    "        print(f\"Weighted average entropies: {w_entropies}, (avg: {np.mean(w_entropies):.3f})\")\n",
    "    return CM, (w_class_entropy, w_cluster_entropy, w_entropies)\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = os.environ.get('DATASET_PATH') + \"pb1data_XW_8358.txt\"\n",
    "    data, labels = load_data(file_path)\n",
    "    data = preprocess_images(data)\n",
    "    data_2d = process_data_tsne(data)\n",
    "    clustered_labels = perform_clustering(data_2d)\n",
    "    counts = Counter(clustered_labels)\n",
    "    print(\"Cluster size distribution:\")\n",
    "    for c in sorted(counts):\n",
    "        print(f\"Cluster {c}: {counts[c]} samples\")\n",
    "    CM, (cs_e, cr_e, we) = evaluate(labels, clustered_labels)\n",
    "    print(\"Confusion Matrix: \\n\", CM)\n",
    "    print(\"Weighted Class Entropies: \", cs_e)\n",
    "    print(\"Weighted Cluster Entropies: \", cr_e)\n",
    "    print(\"Weighted Entropies: \", we)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
