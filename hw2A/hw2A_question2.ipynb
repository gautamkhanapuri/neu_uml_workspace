{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading... t10k-images-idx3-ubyte\n",
      "Reading... t10k-labels-idx1-ubyte\n",
      "Reading... train-images-idx3-ubyte\n",
      "Reading... train-labels-idx1-ubyte\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = os.environ.get(\"MNIST_DATA_PATH\")\n",
    "data_files = os.listdir(data_folder_path)\n",
    "mnist_data_files = [file for file in data_files if file.endswith(\"ubyte\")]\n",
    "\n",
    "def convert_to_int(byts):\n",
    "    integer = int(codecs.encode(byts, 'hex'), 16)\n",
    "    return integer\n",
    "\n",
    "dataset = {}\n",
    "for file in mnist_data_files:\n",
    "    print(\"Reading...\", file)\n",
    "    with open(data_folder_path + file, \"rb\") as f:\n",
    "        data = f.read()\n",
    "        type_of_data = convert_to_int(data[:4])\n",
    "        length = convert_to_int(data[4:8])\n",
    "        if type_of_data == 2051:\n",
    "            category = \"images\"\n",
    "            number_of_rows = convert_to_int(data[8:12])\n",
    "            number_of_columns = convert_to_int(data[12:16])\n",
    "            parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "            parsed = parsed.reshape(length, number_of_rows, number_of_columns);\n",
    "        \n",
    "        if type_of_data == 2049:\n",
    "            category = \"labels\"\n",
    "            parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "            parsed = parsed.reshape(length)\n",
    "\n",
    "        if length == 60000:\n",
    "            set_type = \"train\"\n",
    "        if length == 10000:\n",
    "            set_type = \"test\"\n",
    "    dataset[set_type + '_' + category] = parsed\n",
    "\n",
    "print(dataset['train_images'][0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = dataset['train_images']\n",
    "train_images_flattened = train_images.reshape(60000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_k_centroids(k):\n",
    "    np.random.seed(96)\n",
    "    cen = train_images_flattened.shape[0]\n",
    "    centroids = np.random.choice(cen, k, replace=False)\n",
    "    return train_images_flattened[centroids]\n",
    "\n",
    "\n",
    "def calculate_distances(centroids):\n",
    "    distances = np.linalg.norm(train_images_flattened[:, np.newaxis] - centroids, axis=2)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def assign_clusters(centroids):\n",
    "    distances = calculate_distances(centroids)\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "def update_centroids(labels, k):\n",
    "    new_centroids = np.zeros((k, train_images_flattened.shape[1]))\n",
    "    for i in range(k):\n",
    "        cluster_points = train_images_flattened[labels == i]\n",
    "        if len(cluster_points) > 0:\n",
    "            new_centroids[i] = cluster_points.mean(axis=0)\n",
    "    return new_centroids\n",
    "\n",
    "\n",
    "def k_means(k, max_iterations=100, tols=1e-4):\n",
    "    centroids = initialize_k_centroids(k)\n",
    "\n",
    "    for iterations in range(max_iterations):\n",
    "        labels = assign_clusters(centroids)\n",
    "        new_centroids = update_centroids(labels, k)\n",
    "\n",
    "        if np.linalg.norm(new_centroids - centroids) < tols:\n",
    "            print(f\"Converged in {iterations + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 61 iterations.\n",
      "(10, 784)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02067368 0.04526822 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00210526 0.04547368 0.04547368 0.00189474 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "(60000,)\n",
      "[5 0 9 2 7 6 2 5 2 7]\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "centroids, labels = k_means(k)\n",
    "\n",
    "print(centroids.shape)\n",
    "print(centroids[:, :20])\n",
    "\n",
    "print(labels.shape)\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_variance(centroids, labels):\n",
    "    total_variance = 0.0\n",
    "\n",
    "    for k in range(centroids.shape[0]):\n",
    "        cluster_points = train_images_flattened[labels == k]\n",
    "        \n",
    "        squared_distances = np.sum((cluster_points - centroids[k])**2, axis=1)\n",
    "        \n",
    "        total_variance += np.sum(squared_distances)\n",
    "\n",
    "    return total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of objective function (variance) at k = 10 is 153760292292.6878\n"
     ]
    }
   ],
   "source": [
    "variance_k_10 = calculate_total_variance(centroids, labels)\n",
    "print(\"Value of objective function (variance) at k = 10 is\", variance_k_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying KMeans at k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 49 iterations.\n",
      "(5, 784)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01167099 0.04353464 0.02000741 0.00083364 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "(60000,)\n",
      "[4 0 1 2 1 3 2 4 2 1]\n",
      "Value of objective function (variance) at k = 5 is 169452578273.97925\n"
     ]
    }
   ],
   "source": [
    "k_5 = 5\n",
    "centroids_5, labels_5 = k_means(k_5)\n",
    "\n",
    "print(centroids_5.shape)\n",
    "print(centroids_5[:, :20])\n",
    "\n",
    "print(labels_5.shape)\n",
    "print(labels_5[:10])\n",
    "\n",
    "variance_k_5 = calculate_total_variance(centroids_5, labels_5)\n",
    "print(\"Value of objective function (variance) at k = 5 is\", variance_k_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying KMeans at k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 61 iterations.\n",
      "(10, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "k_20 = 20\n",
    "centroids_20, labels_20 = k_means(10)\n",
    "\n",
    "print(centroids_20.shape)\n",
    "# print(centroids_20[:, :20])\n",
    "\n",
    "print(labels_20.shape)\n",
    "# print(labels_20[:10])\n",
    "\n",
    "# variance_k_20 = calculate_total_variance(centroids_20, labels_20)\n",
    "# print(\"Value of objective function (variance) at k = 20 is\", variance_k_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "print(centroids_20.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "153760292292.6878\n",
    "169452578273.97925\n",
    "153760292292.6878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_purity(original_labels, kmeans_labels, k):\n",
    "    mode_counts = 0\n",
    "    for i in range(k):\n",
    "        cluster_k = original_labels[kmeans_labels == i]\n",
    "        if len(cluster_k) == 0:\n",
    "            continue\n",
    "        unique, counts = np.unique(cluster_k, return_counts=True)\n",
    "        mode_c = np.max(counts)\n",
    "        mode_counts += mode_c\n",
    "\n",
    "    purity_index = mode_counts / 11314 #60000\n",
    "    return purity_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6369833333333333\n"
     ]
    }
   ],
   "source": [
    "orig_labels = dataset['train_labels']\n",
    "\n",
    "purity_index = calculate_purity(orig_labels, labels, 10)\n",
    "print(purity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(original_labels, kmeans_labels, k):\n",
    "    total_gini = 0\n",
    "    for i in range(k):\n",
    "        cluster_k = original_labels[kmeans_labels == i]\n",
    "        dp_in_cluster_k = cluster_k.shape[0]\n",
    "        if dp_in_cluster_k == 0:\n",
    "            continue\n",
    "        unique, counts = np.unique(cluster_k, return_counts=True)\n",
    "        cluster_ks_gini = 0\n",
    "        for count in counts:\n",
    "            cluster_ks_gini += ((count / dp_in_cluster_k) ** 2)\n",
    "        gini_k = 1 - cluster_ks_gini\n",
    "        gini_k = gini_k * dp_in_cluster_k\n",
    "        total_gini += gini_k\n",
    "    final = total_gini / 11314\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4954065677981258\n"
     ]
    }
   ],
   "source": [
    "mnist_gini_index = calculate_gini(orig_labels, labels, 10)\n",
    "print(mnist_gini_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_k_means(main_dataset, k, beta, max_iters=100, threshold=1e-5):\n",
    "    N, D = main_dataset.shape\n",
    "    centroids = np.random.choice(N, k, replace=False)\n",
    "    centroids = main_dataset[centroids]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        dist = cdist(main_dataset, centroids, 'euclidean')\n",
    "\n",
    "        max_dist = np.max(-beta * dist, axis=1, keepdims=True)\n",
    "        log_sum_exp = max_dist + np.log(np.sum(np.exp(-beta * dist - max_dist), axis=1, keepdims=True))\n",
    "        \n",
    "        responsibilities = np.exp(-beta * dist - log_sum_exp)\n",
    "        responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "        denominators = responsibilities.T.sum(axis=1, keepdims=True)\n",
    "        denominators[denominators == 0] = 1e-10\n",
    "        new_centroids = (responsibilities.T @ main_dataset) / denominators\n",
    "\n",
    "        if np.linalg.norm(new_centroids - centroids) < threshold:\n",
    "            print(f\"Converged in {i + 1} iterations.\")\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids, responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_purity_soft(original_labels, responsibilities):\n",
    "    n = len(original_labels)\n",
    "    assigned_clusters = np.argmax(responsibilities, axis=1)\n",
    "\n",
    "    mode_counts = 0\n",
    "    k = responsibilities.shape[1]\n",
    "    for i in range(k):\n",
    "        cluster_k = original_labels[assigned_clusters == i]\n",
    "        if len(cluster_k) == 0:\n",
    "            continue\n",
    "        unique, counts = np.unique(cluster_k, return_counts=True)\n",
    "        mode_c = np.max(counts)\n",
    "        mode_counts += mode_c\n",
    "\n",
    "    purity_index = mode_counts / n\n",
    "    return purity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini_soft(original_labels, responsibilities):\n",
    "    n = len(original_labels)\n",
    "    k = responsibilities.shape[1]\n",
    "\n",
    "    total_gini = 0\n",
    "    for i in range(k):\n",
    "        cluster_probs = responsibilities[:, i]\n",
    "        if np.sum(cluster_probs) == 0:\n",
    "            continue\n",
    "        \n",
    "        weighted_counts = {}\n",
    "        for label, prob in zip(original_labels, cluster_probs):\n",
    "            if label not in weighted_counts:\n",
    "                weighted_counts[label] = 0\n",
    "            weighted_counts[label] += prob\n",
    "\n",
    "        cluster_ks_gini = 0\n",
    "        total_weight = np.sum(cluster_probs)\n",
    "        for count in weighted_counts.values():\n",
    "            cluster_ks_gini += (count / total_weight) ** 2\n",
    "        \n",
    "        gini_k = 1 - cluster_ks_gini\n",
    "        gini_k = gini_k * total_weight\n",
    "        total_gini += gini_k\n",
    "    \n",
    "    final = total_gini / n\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_labels = dataset['train_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta=0.1, Final Centers:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Purity: 0.5906\n",
      "Gini: 0.5503717216074996\n",
      "Beta=1.0, Final Centers:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Purity: 0.5947333333333333\n",
      "Gini: 0.5468512155258705\n",
      "Beta=10.0, Final Centers:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Purity: 0.5774333333333334\n",
      "Gini: 0.555427468337574\n"
     ]
    }
   ],
   "source": [
    "for beta in [0.1, 1.0, 10.0]:\n",
    "        centers, responsibilities = soft_k_means(train_images_flattened, 10, beta)\n",
    "        print(f\"Beta={beta}, Final Centers:\\n\", centers)\n",
    "        p = calculate_purity_soft(orig_labels, responsibilities)\n",
    "        print(f\"Purity: {p}\")\n",
    "        g = calculate_gini_soft(orig_labels, responsibilities)\n",
    "        print(f\"Gini: {g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "dict_keys(['test_images', 'test_labels', 'train_images', 'train_labels'])\n"
     ]
    }
   ],
   "source": [
    "path_to_fmnist_dataset = os.environ.get(\"FASHION_MNIST_DATA_PATH\")\n",
    "files = os.listdir(path_to_fmnist_dataset)\n",
    "fmnist_files = [x for x in files if x.endswith(\"ubyte\")]\n",
    "\n",
    "fmnist_dataset = {}\n",
    "\n",
    "def convert_to_int(byts):\n",
    "    integer = int(codecs.encode(byts, 'hex'), 16)\n",
    "    return integer\n",
    "\n",
    "for file in fmnist_files:\n",
    "    with open(path_to_fmnist_dataset + file, 'rb') as fd:\n",
    "        fmnist_data = fd.read()\n",
    "\n",
    "        category = convert_to_int(fmnist_data[:4])\n",
    "        length = convert_to_int(fmnist_data[4:8])\n",
    "        if category == 2051:\n",
    "            category = \"images\"\n",
    "            no_of_rows = convert_to_int(fmnist_data[8: 12])\n",
    "            no_of_cols = convert_to_int(fmnist_data[12: 16])\n",
    "            parsed = np.frombuffer(fmnist_data, dtype=np.uint8, offset=16)\n",
    "            parsed = parsed.reshape(length, no_of_rows, no_of_cols)\n",
    "        if category == 2049:\n",
    "            category = \"labels\"\n",
    "            parsed = np.frombuffer(fmnist_data, dtype=np.uint8, offset=8)\n",
    "            parsed = parsed.reshape(length)\n",
    "        if length == 60000:\n",
    "            set_type = \"train\"\n",
    "        if length == 10000:\n",
    "            set_type = \"test\"\n",
    "    fmnist_dataset[set_type + \"_\" + category] = parsed\n",
    "\n",
    "print(fmnist_dataset['train_images'][:5])\n",
    "print(fmnist_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_train_images = fmnist_dataset['train_images']\n",
    "fmnist_train_images_flattened = fmnist_train_images.reshape(fmnist_train_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_k_centroids(flattened_images_dataset, k):\n",
    "    np.random.seed(97)\n",
    "    centroids = np.random.choice(flattened_images_dataset.shape[0], size=k, replace=False)\n",
    "    return flattened_images_dataset[centroids]\n",
    "\n",
    "\n",
    "def calculate_distances(flattened_images_dataset, centroids):\n",
    "    reshaped_images_datset = flattened_images_dataset[:, np.newaxis]\n",
    "    minus_centroids = reshaped_images_datset - centroids\n",
    "    dist = np.linalg.norm(minus_centroids, axis=2)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def reassign_clusters(flattened_image_dataset, centroids):\n",
    "    distances = calculate_distances(flattened_image_dataset, centroids)\n",
    "    cluster_labels = np.argmin(distances, axis=1)\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def recalculate_centroids(cluster_labels, flattened_images_dataset, k):\n",
    "    new_centroids = np.zeros((k, flattened_images_dataset.shape[1]))\n",
    "    for i in range(k):\n",
    "        dps_in_cluster_k = flattened_images_dataset[cluster_labels == i]\n",
    "        avg = np.mean(dps_in_cluster_k, axis=0)\n",
    "        new_centroids[i] = avg\n",
    "    return new_centroids\n",
    "\n",
    "def general_k_means(k, flattened_images_dataset, max_iterations=100, threshold=1e-4):\n",
    "    centroids = initialize_k_centroids(flattened_images_dataset, k)\n",
    "    for iteration in range(max_iterations):\n",
    "        cluster_labels = reassign_clusters(flattened_images_dataset, centroids)\n",
    "        new_centroids = recalculate_centroids(cluster_labels, flattened_images_dataset, k)\n",
    "\n",
    "        diff_in_centroids = centroids - new_centroids\n",
    "        diff_in_centroids = np.linalg.norm(diff_in_centroids)\n",
    "        if diff_in_centroids < threshold:\n",
    "            print(f\"Converged in {iteration + 1}th iteration.\")\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 72th iteration.\n",
      "Completed\n",
      "(10, 784)\n",
      "[[1.81370644e-03 3.88651380e-03 2.35781837e-02 7.33255603e-02\n",
      "  2.30081617e-01 2.89933929e-01 4.99287472e-01 1.40743620e+00\n",
      "  3.19238243e+00 7.13499158e+00 1.37442674e+01 2.10883534e+01\n",
      "  2.51936779e+01 2.29187719e+01 2.25083560e+01 2.50352377e+01\n",
      "  2.55494235e+01 1.83579479e+01 1.04183184e+01 5.59709807e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.51287497e-03 4.51287497e-03 4.51287497e-03 4.51287497e-03\n",
      "  5.84019113e-03 4.51287497e-03 2.99973454e-02 2.65463233e-02\n",
      "  8.22936023e-03 4.51287497e-03 4.51287497e-03 4.51287497e-03\n",
      "  3.05282718e-02 2.65463233e-02 4.51287497e-03 6.37111760e-03]\n",
      " [2.42336120e-04 6.66424331e-03 4.78613837e-02 2.21616382e-01\n",
      "  4.71707258e-01 7.68569005e-01 1.96958682e+00 5.12686296e+00\n",
      "  1.11101418e+01 2.12935902e+01 3.52104689e+01 5.08641706e+01\n",
      "  6.48262450e+01 6.66003877e+01 6.96165031e+01 7.11936266e+01\n",
      "  6.28343633e+01 4.49200291e+01 2.88854962e+01 1.60631286e+01]\n",
      " [0.00000000e+00 3.90742411e-03 3.90742411e-03 3.90742411e-03\n",
      "  3.90742411e-03 3.90742411e-03 3.90742411e-03 3.90742411e-03\n",
      "  3.90742411e-03 3.90742411e-03 3.90742411e-03 3.90742411e-03\n",
      "  3.90742411e-03 3.90742411e-03 3.30628194e-03 4.50856628e-03\n",
      "  4.50856628e-03 3.30628194e-03 3.90742411e-03 3.90742411e-03]\n",
      " [3.51161534e-03 1.12101567e-02 8.06320908e-02 1.52890330e-01\n",
      "  2.41085900e-01 5.26337115e-01 1.20056726e+00 4.93962723e+00\n",
      "  1.46103458e+01 3.24490816e+01 5.74986494e+01 7.32821448e+01\n",
      "  7.32458131e+01 6.53980281e+01 6.31839546e+01 7.12826850e+01\n",
      "  7.69062669e+01 6.84823069e+01 4.47579687e+01 2.20887358e+01]]\n",
      "(60000,)\n",
      "[3 4 0 2 7 9 5 9 0 0]\n"
     ]
    }
   ],
   "source": [
    "fmnist_k_10 = 10\n",
    "fmnist_centroids, fmnist_labels = general_k_means(fmnist_k_10, fmnist_train_images_flattened)\n",
    "\n",
    "print(\"Completed\")\n",
    "\n",
    "print(fmnist_centroids.shape)\n",
    "print(fmnist_centroids[:5, :20])\n",
    "\n",
    "print(fmnist_labels.shape)\n",
    "print(fmnist_labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5241166666666667\n"
     ]
    }
   ],
   "source": [
    "fmnist_purity = calculate_purity(fmnist_dataset['train_labels'], fmnist_labels, 10)\n",
    "print(fmnist_purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5900829479702764\n"
     ]
    }
   ],
   "source": [
    "fmnist_gini = calculate_gini(fmnist_dataset['train_labels'], fmnist_labels, 10)\n",
    "print(fmnist_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 64th iteration.\n",
      "Completed\n",
      "(5, 784)\n",
      "[[1.34544231e-03 5.71812984e-03 4.49041372e-02 1.78187016e-01\n",
      "  4.49545913e-01 6.47157753e-01 1.44542550e+00 3.76639758e+00\n",
      "  8.24117053e+00 1.71300875e+01 3.13654558e+01 4.51570804e+01\n",
      "  5.45450723e+01 5.37599226e+01 5.55428019e+01 5.75831652e+01\n",
      "  5.39743525e+01 3.97001345e+01 2.45809788e+01 1.32411705e+01]\n",
      " [0.00000000e+00 0.00000000e+00 6.21007807e-04 4.52448545e-03\n",
      "  4.16962385e-03 1.33073101e-03 1.24201561e-03 1.24201561e-03\n",
      "  1.68559262e-03 3.54861604e-03 1.23314407e-02 2.41305891e-02\n",
      "  1.99609652e-02 1.68559262e-02 2.49290277e-02 1.84528034e-02\n",
      "  2.49290277e-02 1.54364798e-02 5.76650106e-03 3.63733144e-03]\n",
      " [2.11445331e-03 9.95839302e-03 6.55480527e-02 2.48755201e-01\n",
      "  5.55419139e-01 1.00115954e+00 1.80328763e+00 4.51899598e+00\n",
      "  1.06874702e+01 2.20622059e+01 4.20225087e+01 6.70118682e+01\n",
      "  8.84813451e+01 8.72486870e+01 8.64341450e+01 9.09075097e+01\n",
      "  8.22304754e+01 5.34299843e+01 2.96832413e+01 1.44903485e+01]\n",
      " [1.06541658e-04 1.39569572e-02 2.51438312e-02 1.86447901e-02\n",
      "  5.03942041e-02 4.71979544e-02 7.24483273e-02 1.05689325e-01\n",
      "  1.15704240e-01 2.05518858e-01 3.07692308e-01 5.84594076e-01\n",
      "  2.41167697e+00 7.02322608e+00 1.04797571e+01 1.01257192e+01\n",
      "  7.03302791e+00 4.68676753e+00 3.71809077e+00 3.30769231e+00]\n",
      " [0.00000000e+00 1.56384393e-04 5.23887716e-03 1.84533584e-02\n",
      "  7.60028149e-02 1.60372195e-01 3.89318946e-01 1.67604973e+00\n",
      "  6.73406834e+00 2.63464696e+01 7.70380796e+01 9.61674877e+01\n",
      "  9.04542967e+01 8.34826022e+01 7.85833138e+01 8.76974744e+01\n",
      "  9.21025882e+01 8.91063414e+01 6.29209477e+01 1.74777543e+01]]\n",
      "(60000,)\n",
      "[3 2 0 0 4 2 1 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "fmnist_k_5 = 5\n",
    "fmnist_centroids_5, fmnist_labels_5 = general_k_means(fmnist_k_5, fmnist_train_images_flattened)\n",
    "\n",
    "print(\"Completed\")\n",
    "\n",
    "print(fmnist_centroids_5.shape)\n",
    "print(fmnist_centroids_5[:5, :20])\n",
    "\n",
    "print(fmnist_labels_5.shape)\n",
    "print(fmnist_labels_5[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fmnist_k_20 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 2\u001b[0m fmnist_centroids_20, fmnist_labels_20 \u001b[38;5;241m=\u001b[39m \u001b[43mgeneral_k_means\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmnist_k_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmnist_train_images_flattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(fmnist_centroids_20\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m, in \u001b[0;36mgeneral_k_means\u001b[0;34m(k, flattened_images_dataset, max_iterations, threshold)\u001b[0m\n\u001b[1;32m     29\u001b[0m centroids \u001b[38;5;241m=\u001b[39m initialize_k_centroids(flattened_images_dataset, k)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n\u001b[0;32m---> 31\u001b[0m     cluster_labels \u001b[38;5;241m=\u001b[39m \u001b[43mreassign_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_images_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     new_centroids \u001b[38;5;241m=\u001b[39m recalculate_centroids(cluster_labels, flattened_images_dataset, k)\n\u001b[1;32m     34\u001b[0m     diff_in_centroids \u001b[38;5;241m=\u001b[39m centroids \u001b[38;5;241m-\u001b[39m new_centroids\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mreassign_clusters\u001b[0;34m(flattened_image_dataset, centroids)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreassign_clusters\u001b[39m(flattened_image_dataset, centroids):\n\u001b[0;32m---> 15\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_image_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     cluster_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(distances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cluster_labels\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mcalculate_distances\u001b[0;34m(flattened_images_dataset, centroids)\u001b[0m\n\u001b[1;32m      8\u001b[0m reshaped_images_datset \u001b[38;5;241m=\u001b[39m flattened_images_dataset[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m      9\u001b[0m minus_centroids \u001b[38;5;241m=\u001b[39m reshaped_images_datset \u001b[38;5;241m-\u001b[39m centroids\n\u001b[0;32m---> 10\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminus_centroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[0;32m~/neu_uml_workspace/chatbot_env/lib/python3.9/site-packages/numpy/linalg/linalg.py:2583\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mord\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2581\u001b[0m     \u001b[38;5;66;03m# special case for speedup\u001b[39;00m\n\u001b[1;32m   2582\u001b[0m     s \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mconj() \u001b[38;5;241m*\u001b[39m x)\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m-> 2583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(\u001b[43madd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;66;03m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[1;32m   2585\u001b[0m \u001b[38;5;66;03m# are valid for vectors\u001b[39;00m\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mord\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fmnist_k_20 = 20\n",
    "fmnist_centroids_20, fmnist_labels_20 = general_k_means(fmnist_k_20, fmnist_train_images_flattened)\n",
    "\n",
    "print(\"Completed\")\n",
    "\n",
    "print(fmnist_centroids_20.shape)\n",
    "print(fmnist_centroids_20[:5, :20])\n",
    "\n",
    "print(fmnist_labels_20.shape)\n",
    "print(fmnist_labels_20[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents (11314, 1000)\n",
      "labels (11314,)\n",
      "Converged in 60th iteration\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset=\"train\")\n",
    "documents = newsgroups.data\n",
    "document_labels = newsgroups.target\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "vectors = tfidf_vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "print(\"documents\", vectors.shape)\n",
    "print(\"labels\", document_labels.shape)\n",
    "\n",
    "\n",
    "def initialize_centroids(data, k):\n",
    "    indices = np.random.choice(data.shape[0], k, replace=False)\n",
    "    return data[indices]\n",
    "\n",
    "\n",
    "def assign_clusters(data, centroids):\n",
    "    distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "def update_centroids(data, labels, k):\n",
    "    centroids = np.zeros((k, data.shape[1]))\n",
    "    for i in range(k):\n",
    "        cluster_points = data[labels == i]\n",
    "        if cluster_points.shape[0] > 0:\n",
    "            centroids[i] = np.mean(cluster_points, axis=0)\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def kmeans(data, k, max_iters=100, tol=1e-4):\n",
    "    centroids = initialize_centroids(data, k)\n",
    "    for i in range(max_iters):\n",
    "        labels = assign_clusters(data, centroids)\n",
    "        new_centroids = update_centroids(data, labels, k)\n",
    "        if np.allclose(centroids, new_centroids, atol=tol):\n",
    "            print(f\"Converged in {i + 1}th iteration\")\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return labels, centroids\n",
    "\n",
    "\n",
    "ng_k = 20\n",
    "ng_labels, ng_centroids = kmeans(vectors, ng_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8806129460379966\n"
     ]
    }
   ],
   "source": [
    "newsgroups_gini_index = calculate_gini(document_labels, ng_labels, 20)\n",
    "print(newsgroups_gini_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2121265688527488\n"
     ]
    }
   ],
   "source": [
    "newsgroups_purity = calculate_purity(document_labels, ng_labels, 20)\n",
    "print(newsgroups_purity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
