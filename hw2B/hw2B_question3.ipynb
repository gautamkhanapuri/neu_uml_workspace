{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 NG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"alt.atheism\", \"sci.med\", \"sci.electronics\", \"comp.graphics\", \"talk.politics.guns\", \"sci.crypt\"]\n",
    "documents = fetch_20newsgroups(categories=categories).data\n",
    "labels_orig = fetch_20newsgroups(categories=categories).target\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "vectors = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "class DBSCAN:\n",
    "\n",
    "    def __init__(self, epsilon, min_pts):\n",
    "        self.epsilon = epsilon\n",
    "        self.min_pts = min_pts\n",
    "        self.labels = None\n",
    "\n",
    "    def _region_query(self, dataset, i):\n",
    "        similarities = cosine_similarity(dataset[i].reshape(1, -1), dataset)[0]\n",
    "        # print(f\"Point {i}: max={np.max(similarities)}, min={np.min(similarities)}, mean={np.mean(similarities)}\")\n",
    "        return np.where(similarities > self.epsilon)[0]\n",
    "    \n",
    "    def _expand_cluster(self, dataset, i, neighbors, cluster_id):\n",
    "        self.labels[i] = cluster_id\n",
    "        queue = deque(neighbors)\n",
    "\n",
    "        while queue:\n",
    "            index = queue.popleft()\n",
    "            if self.labels[index] == -2:\n",
    "                self.labels[index] = cluster_id\n",
    "            elif self.labels[index] == -1:\n",
    "                self.labels[index] = cluster_id\n",
    "                current_neighbors = self._region_query(dataset, index)\n",
    "                if len(current_neighbors) >= self.min_pts:\n",
    "                    queue.extend(current_neighbors)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        n = dataset.shape[0]\n",
    "        self.labels = np.full(n, -1)\n",
    "        cluster_id = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            if self.labels[i] == -1:\n",
    "                neighbours = self._region_query(dataset, i)\n",
    "                if len(neighbours) < self.min_pts:\n",
    "                    self.labels[i] = -2\n",
    "                else:\n",
    "                    self._expand_cluster(dataset, i, neighbours, cluster_id)\n",
    "                    cluster_id += 1\n",
    "\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_20ng = DBSCAN(epsilon=0.85, min_pts=3)\n",
    "labels = dbscan_20ng.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n",
      " 23 24 25 26 27]\n",
      "[3290    3    3    4    3    3    7    5    6    4    3    3    4    4\n",
      "    4    3    3    3    3    3    4    3    4    3    3    3    3    3\n",
      "    3]\n"
     ]
    }
   ],
   "source": [
    "label, count = np.unique(labels, return_counts=True)\n",
    "print(label)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.18976953977916106\n"
     ]
    }
   ],
   "source": [
    "score = silhouette_score(vectors, labels, metric='cosine')\n",
    "print(\"Silhouette Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "dict_keys(['test_images', 'test_labels', 'train_images', 'train_labels'])\n"
     ]
    }
   ],
   "source": [
    "path_to_fmnist_dataset = os.environ.get(\"FASHION_MNIST_DATA_PATH\")\n",
    "files = os.listdir(path_to_fmnist_dataset)\n",
    "fmnist_files = [x for x in files if x.endswith(\"ubyte\")]\n",
    "\n",
    "fmnist_dataset = {}\n",
    "\n",
    "def convert_to_int(byts):\n",
    "    integer = int(codecs.encode(byts, 'hex'), 16)\n",
    "    return integer\n",
    "\n",
    "for file in fmnist_files:\n",
    "    with open(path_to_fmnist_dataset + file, 'rb') as fd:\n",
    "        fmnist_data = fd.read()\n",
    "\n",
    "        category = convert_to_int(fmnist_data[:4])\n",
    "        length = convert_to_int(fmnist_data[4:8])\n",
    "        if category == 2051:\n",
    "            category = \"images\"\n",
    "            no_of_rows = convert_to_int(fmnist_data[8: 12])\n",
    "            no_of_cols = convert_to_int(fmnist_data[12: 16])\n",
    "            parsed = np.frombuffer(fmnist_data, dtype=np.uint8, offset=16)\n",
    "            parsed = parsed.reshape(length, no_of_rows, no_of_cols)\n",
    "        if category == 2049:\n",
    "            category = \"labels\"\n",
    "            parsed = np.frombuffer(fmnist_data, dtype=np.uint8, offset=8)\n",
    "            parsed = parsed.reshape(length)\n",
    "        if length == 60000:\n",
    "            set_type = \"train\"\n",
    "        if length == 10000:\n",
    "            set_type = \"test\"\n",
    "    fmnist_dataset[set_type + \"_\" + category] = parsed\n",
    "\n",
    "print(fmnist_dataset['train_images'][:5])\n",
    "print(fmnist_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_train_images = fmnist_dataset['train_images']\n",
    "fmnist_train_images_flattened = fmnist_train_images.reshape(fmnist_train_images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANImages:\n",
    "\n",
    "    def __init__(self, epsilon, min_pts):\n",
    "        self.epsilon = epsilon\n",
    "        self.min_pts = min_pts\n",
    "        self.labels = None\n",
    "\n",
    "    def _region_query(self, dataset, i):\n",
    "        return np.where(self.dist_matrix[i] < self.epsilon)[0]\n",
    "    \n",
    "    def _expand_cluster(self, dataset, i, neighbors, cluster_id):\n",
    "        self.labels[i] = cluster_id\n",
    "        queue = deque(neighbors)\n",
    "\n",
    "        while queue:\n",
    "            index = queue.popleft()\n",
    "            if self.labels[index] == -2:\n",
    "                self.labels[index] = cluster_id\n",
    "            elif self.labels[index] == -1:\n",
    "                self.labels[index] = cluster_id\n",
    "                current_neighbors = self._region_query(dataset, index)\n",
    "                if len(current_neighbors) >= self.min_pts:\n",
    "                    queue.extend(current_neighbors)\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        n = dataset.shape[0]\n",
    "        self.labels = np.full(n, -1)\n",
    "        self.dist_matrix = squareform(pdist(dataset, metric='euclidean'))\n",
    "        cluster_id = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            # print(f\"Point {i}: max={np.max(self.dist_matrix)}, min={np.min(self.dist_matrix)}, mean={np.mean(self.dist_matrix)}\")            \n",
    "            if self.labels[i] == -1:\n",
    "                neighbours = self._region_query(dataset, i)\n",
    "                if len(neighbours) < self.min_pts:\n",
    "                    self.labels[i] = -2\n",
    "                else:\n",
    "                    self._expand_cluster(dataset, i, neighbours, cluster_id)\n",
    "                    cluster_id += 1\n",
    "\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fmnist_data = fmnist_train_images_flattened[np.random.choice(60000, 20000, replace=False)]\n",
    "sample_fmnist_data = sample_fmnist_data/255\n",
    "dbscan_fmnist = DBSCANImages(7, 2)\n",
    "labels_fmnist = dbscan_fmnist.fit(sample_fmnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, c1 = np.unique(labels_fmnist, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  0  1  2  3  4  5  6]\n",
      "[  293 19694     2     3     2     2     2     2]\n",
      "Silhouette Score: -0.027808517073190483\n"
     ]
    }
   ],
   "source": [
    "print(l1)\n",
    "print(c1)\n",
    "score1 = silhouette_score(sample_fmnist_data, labels_fmnist, metric='euclidean')\n",
    "print(\"Silhouette Score:\", score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = os.environ.get(\"DATASET_PATH\") + \"household_power_consumption.txt\"\n",
    "df = pd.read_csv(file_path, sep=\";\", nrows=1, low_memory=False)\n",
    "\n",
    "sample_data = pd.read_csv(file_path, sep=\";\", skiprows=lambda x: x > 1 and np.random.rand() > (20000 / 2075259), low_memory=False, na_values=[\"?\"], names=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Time', 'Global_active_power', 'Global_reactive_power',\n",
      "       'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2',\n",
      "       'Sub_metering_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(sample_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.drop([\"Date\", \"Time\"],axis=1, inplace=True)\n",
    "\n",
    "sample_data = sample_data.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "sample_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.3125562015528818\n"
     ]
    }
   ],
   "source": [
    "dbscan_uci = DBSCANImages(2.5, 13)\n",
    "labels_uci = dbscan_uci.fit(sample_data)\n",
    "l2, c2 = np.unique(labels_uci, return_counts=True)\n",
    "\n",
    "print(l2)\n",
    "print(c2)\n",
    "s2 = silhouette_score(sample_data, labels_uci, metric=\"euclidean\")\n",
    "print(f\"Silhouette Score: {s2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
